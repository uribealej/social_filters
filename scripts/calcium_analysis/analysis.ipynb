{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "from scipy.stats import gaussian_kde\n",
    "from numpy.polynomial.polynomial import Polynomial\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "78495381164a8af2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def filter_dim_rois(fluo, sigma_thresh=2):\n",
    "    \"\"\"Remove dim ROIs based on mean fluorescence.\"\"\"\n",
    "    mean_f = np.mean(fluo, axis=0)\n",
    "    thresh = np.mean(mean_f) - sigma_thresh * np.std(mean_f)\n",
    "    mask = mean_f >= thresh\n",
    "    removed = np.where(~mask)[0]\n",
    "    kept = np.where(mask)[0]\n",
    "    fluo_filtered = fluo[:, mask]\n",
    "\n",
    "    print(f\"Removed {len(removed)} dim ROIs: {removed.tolist()}\")\n",
    "    return fluo_filtered, kept, removed\n",
    "\n",
    "def compute_percentile_baseline(fluo, fps, tau, percentile=8, instability_ratio=0.1):\n",
    "    \"\"\"Compute F0 baseline and discard unstable ROIs.\"\"\"\n",
    "    T, N = fluo.shape\n",
    "    w = int(max(15, 40 * tau) * fps)\n",
    "    F0 = np.full_like(fluo, np.nan)\n",
    "    unstable_indices = []\n",
    "\n",
    "    for n in range(N):\n",
    "        trace = fluo[:, n]\n",
    "        baseline = np.zeros_like(trace)\n",
    "        for t in range(w, T - w):\n",
    "            baseline[t] = np.percentile(trace[t - w:t + w + 1], percentile)\n",
    "        baseline[:w] = baseline[w]\n",
    "        baseline[-w:] = baseline[-w - 1]\n",
    "        if np.min(baseline) < instability_ratio * np.max(baseline):\n",
    "            unstable_indices.append(n)\n",
    "            continue\n",
    "        F0[:, n] = uniform_filter1d(baseline, size=w)\n",
    "    return F0, unstable_indices\n",
    "\n",
    "def filter_inactive_rois_by_std(deltaF_F, final_kept, final_removed, min_std=0.01, verbose=True):\n",
    "    \"\"\"\n",
    "    Remove ROIs with low std in ΔF/F.\n",
    "    \"\"\"\n",
    "    stds = np.std(deltaF_F, axis=0)\n",
    "    mean_std = np.mean(stds)  # compute mean of stds\n",
    "    active_mask = stds > min_std\n",
    "    inactive_indices = np.where(~active_mask)[0]\n",
    "    deltaF_F_active = deltaF_F[:, active_mask]\n",
    "\n",
    "    # Map inactive indices back to original fluo_raw indexing\n",
    "    activity_removed = final_kept[inactive_indices]\n",
    "\n",
    "    # Update removed and kept indices in fluo_raw space\n",
    "    final_removed_all = np.sort(np.unique(np.concatenate([final_removed, activity_removed])))\n",
    "    final_kept_active = final_kept[active_mask]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Mean std across all ROIs: {mean_std:.5f}\")\n",
    "        print(f\"Removed {len(inactive_indices)} inactive ROIs (std < {min_std})\")\n",
    "        print(f\"Remaining: {deltaF_F_active.shape[1]} ROIs\")\n",
    "        print(f\"Total removed (cumulative): {len(final_removed_all)}\")\n",
    "\n",
    "    return deltaF_F_active, final_kept_active, final_removed_all\n",
    "\n",
    "def compute_deltaF_F(\n",
    "    fluo_raw,\n",
    "    fps,\n",
    "    tau,\n",
    "    percentile=8,\n",
    "    instability_ratio=0.1,\n",
    "    sigma_thresh=2,\n",
    "    min_std=0.01,\n",
    "    verbose=True\n",
    "):\n",
    "    # Step 1: Filter dim ROIs\n",
    "    fluo_filtered, kept_rois, dim_removed = filter_dim_rois(fluo_raw, sigma_thresh)\n",
    "\n",
    "    # Step 2: Compute baseline on filtered data\n",
    "    F0, unstable_local = compute_percentile_baseline(fluo_filtered, fps, tau, percentile, instability_ratio)\n",
    "\n",
    "    # Step 3: Determine which of the dim-kept ROIs are unstable\n",
    "    unstable_mask = np.isnan(F0).all(axis=0)\n",
    "    unstable_indices = np.where(unstable_mask)[0]\n",
    "    print(f\"Removed {len(unstable_indices)} unstable ROIs: {unstable_indices.tolist()}\")\n",
    "\n",
    "    # Final kept ROIs are those that are both bright enough and stable\n",
    "    final_kept = kept_rois[~unstable_mask]\n",
    "    final_removed = np.sort(np.concatenate([dim_removed, kept_rois[unstable_mask]]))\n",
    "\n",
    "    # Step 4: Keep only stable ROIs\n",
    "    fluo_clean = fluo_filtered[:, ~unstable_mask]\n",
    "    F0_clean = F0[:, ~unstable_mask]\n",
    "    F0_safe = np.where(F0_clean == 0, np.finfo(float).eps, F0_clean)\n",
    "    deltaF_F = (fluo_clean - F0_clean) / F0_safe\n",
    "\n",
    "    # Step 5: Remove low-activity ROIs\n",
    "    deltaF_F_active, final_kept_active, final_removed_all = filter_inactive_rois_by_std(\n",
    "        deltaF_F, final_kept, final_removed, min_std, verbose\n",
    "    )\n",
    "\n",
    "\n",
    "    print(f\"ΔF/F₀ computed. Final number of ROIs: {deltaF_F.shape[1]}\")\n",
    "\n",
    "    return deltaF_F_active, final_kept_active, final_removed_all\n",
    "\n"
   ],
   "id": "d9c1a73d7ad8ef7f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data=np.load(\"C:/Users/suribear/OneDrive - Université de Lausanne/Lab/Data/2p/F.npy\").T\n",
    "\n",
    "deltaF_F, ind_kept, ind_removed = compute_deltaF_F(\n",
    "    fluo_raw=data,\n",
    "    fps=2.0,\n",
    "    tau=3.0,\n",
    "    percentile=8,\n",
    "    instability_ratio=0.1,\n",
    "    sigma_thresh=2,\n",
    "    min_std=0.01\n",
    ")\n"
   ],
   "id": "b236dee02262e15f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def gaussfit_neg(data, peak_threshold_fraction=0.2, trim_std=2.0):\n",
    "    \"\"\"\n",
    "    Robust Gaussian fitting of the negative half of a KDE of the input data.\n",
    "\n",
    "    This method is especially suitable for estimating the noise characteristics\n",
    "    of ΔF/F traces from calcium imaging, where noise is assumed to dominate\n",
    "    the negative side of the distribution.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : ndarray\n",
    "        1D array of ΔF/F values for a single ROI over time.\n",
    "    peak_threshold_fraction : float, optional (default=0.2)\n",
    "        Fraction of the KDE peak height to include in the fit.\n",
    "        Values below this threshold are excluded from the Gaussian fit.\n",
    "    trim_std : float, optional (default=2.0)\n",
    "        If the fit fails, fallback uses data within ±`trim_std` standard deviations\n",
    "        to compute robust sigma.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    sigma : float\n",
    "        Estimated standard deviation of the noise.\n",
    "    mu : float\n",
    "        Estimated mean of the Gaussian fit.\n",
    "    A : float or np.nan\n",
    "        Estimated amplitude of the Gaussian. If fallback is used, A is np.nan.\n",
    "    used_fallback : bool\n",
    "        True if the fallback method was used due to poor fit.\n",
    "    \"\"\"\n",
    "\n",
    "    N = len(data)\n",
    "\n",
    "    # Step 1: Estimate the KDE of the input data\n",
    "    kde = gaussian_kde(data)\n",
    "    x = np.linspace(np.min(data), np.max(data), 200)\n",
    "    y = kde(x)\n",
    "\n",
    "    # Step 2: Find the mode of the distribution (peak of KDE)\n",
    "    ind_peak = np.argmax(y)\n",
    "\n",
    "    # Step 3: Use only the left half of the distribution (assumed to be noise-dominated)\n",
    "    x_fit = x[:ind_peak + 1]\n",
    "    y_fit = y[:ind_peak + 1]\n",
    "\n",
    "    # Keep only the part of the curve above a fraction of the peak\n",
    "    ymax = np.max(y_fit)\n",
    "    keep = y_fit > ymax * peak_threshold_fraction\n",
    "    x_fit = x_fit[keep]\n",
    "    y_fit = y_fit[keep] / N  # normalize to probability density\n",
    "\n",
    "    # Abort if insufficient data to fit\n",
    "    if len(x_fit) < 3:\n",
    "        return np.nan, np.nan, np.nan, True\n",
    "\n",
    "    try:\n",
    "        # Step 4: Fit log(y) = a2 x^2 + a1 x + a0\n",
    "        ylog = np.log(y_fit)\n",
    "        coeffs = np.polyfit(x_fit, ylog, 2)\n",
    "        A2, A1, A0 = coeffs\n",
    "\n",
    "        # Validate parabola direction\n",
    "        if A2 >= 0:\n",
    "            raise ValueError(\"Parabola opens upwards — not a valid Gaussian\")\n",
    "\n",
    "        sigma = np.sqrt(-1 / (2 * A2))\n",
    "        mu = A1 * sigma ** 2\n",
    "        A = np.exp(A0 + mu ** 2 / (2 * sigma ** 2))\n",
    "\n",
    "        if not np.isreal(sigma):\n",
    "            raise ValueError(\"Sigma is complex\")\n",
    "\n",
    "        return float(sigma), float(mu), float(A), False  # fit successful\n",
    "\n",
    "    except Exception:\n",
    "        # Fallback: use robust std within ±trim_std\n",
    "        dev = np.nanstd(data)\n",
    "        mask = np.abs(data) <= trim_std * dev\n",
    "        trimmed = data[mask]\n",
    "        sigma = np.nanstd(trimmed)\n",
    "        mu = np.nanmean(trimmed)\n",
    "        return float(sigma), float(mu), np.nan, True  # fallback used\n",
    "\n",
    "def fit_noise_and_center_traces(deltaF_F, fit_function):\n",
    "    \"\"\"\n",
    "    Apply a Gaussian fit to each ROI's ΔF/F trace (typically using only the negative half)\n",
    "    to estimate noise parameters and center the traces.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    deltaF_F : ndarray of shape (T, N)\n",
    "        Raw ΔF/F data with T timepoints and N ROIs (neurons).\n",
    "    fit_function : callable\n",
    "        Function that takes a 1D ΔF/F trace and returns (sigma, mu, A).\n",
    "        For example: `gaussfit_neg(trace)`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    deltaF_centered : ndarray of shape (T, N)\n",
    "        ΔF/F data with mean noise offset (mu) subtracted per ROI.\n",
    "    sigma_vals : ndarray of shape (N,)\n",
    "        Estimated noise standard deviation per ROI.\n",
    "    mu_vals : ndarray of shape (N,)\n",
    "        Estimated noise mean per ROI.\n",
    "    A_vals : ndarray of shape (N,)\n",
    "        Estimated noise amplitude per ROI (optional use).\n",
    "    \"\"\"\n",
    "    T, N = deltaF_F.shape\n",
    "    sigma_vals = np.zeros(N)\n",
    "    mu_vals = np.zeros(N)\n",
    "    A_vals = np.zeros(N)\n",
    "\n",
    "    for i in range(N):\n",
    "        trace = deltaF_F[:, i]\n",
    "        sigma_vals[i], mu_vals[i], A_vals[i] = fit_function(trace)\n",
    "\n",
    "    # Center the traces by removing estimated noise mean\n",
    "    deltaF_centered = deltaF_F - mu_vals\n",
    "\n",
    "    return deltaF_centered, sigma_vals, mu_vals, A_vals\n",
    "\n"
   ],
   "id": "6c493b54e3be189b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def estimate_and_center_noise_model(deltaF_F, fit_function=gaussfit_neg, verbose=True):\n",
    "    \"\"\"\n",
    "    Fit Gaussian noise model for each ROI using the negative half of the ΔF/F distribution,\n",
    "    then center each ROI's trace by subtracting the estimated mean (μ).\n",
    "\n",
    "    This function is typically used after preprocessing (e.g., ΔF/F calculation)\n",
    "    and helps prepare the data for statistical modeling or event detection.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    deltaF_F : ndarray of shape (T, N)\n",
    "        Preprocessed ΔF/F traces with T timepoints and N ROIs.\n",
    "    fit_function : callable, optional\n",
    "        Fitting function to use for noise modeling (default: gaussfit_neg).\n",
    "        Should return (σ, μ, A, used_fallback) per ROI.\n",
    "    verbose : bool\n",
    "        Whether to print fitting statistics (e.g., number of fallback uses).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    deltaF_centered : ndarray of shape (T, N)\n",
    "        ΔF/F data with estimated μ subtracted per ROI.\n",
    "    sigma_vals : ndarray of shape (N,)\n",
    "        Estimated noise standard deviation (σ) per ROI.\n",
    "    mu_vals : ndarray of shape (N,)\n",
    "        Estimated Gaussian mean (μ) per ROI.\n",
    "    A_vals : ndarray of shape (N,)\n",
    "        Estimated Gaussian amplitude (A) per ROI.\n",
    "    fallback_flags : list of bool\n",
    "        List indicating whether each ROI required fallback estimation.\n",
    "    \"\"\"\n",
    "    T, N = deltaF_F.shape\n",
    "    sigma_vals = np.zeros(N)\n",
    "    mu_vals = np.zeros(N)\n",
    "    A_vals = np.zeros(N)\n",
    "    fallback_flags = []\n",
    "\n",
    "    # --- Fit noise model per ROI ---\n",
    "    for i in range(N):\n",
    "        trace = deltaF_F[:, i]\n",
    "        sigma, mu, A, used_fallback = fit_function(trace)\n",
    "        sigma_vals[i] = sigma\n",
    "        mu_vals[i] = mu\n",
    "        A_vals[i] = A\n",
    "        fallback_flags.append(used_fallback)\n",
    "\n",
    "    # --- Subtract estimated noise mean from each trace ---\n",
    "    deltaF_centered = deltaF_F - mu_vals  # Broadcast subtraction across ROIs\n",
    "\n",
    "    if verbose:\n",
    "        n_fallback = sum(fallback_flags)\n",
    "        print(f\"[Noise Fit] Used fallback on {n_fallback} out of {N} ROIs \"\n",
    "              f\"({100 * n_fallback / N:.1f}%)\")\n",
    "\n",
    "    return deltaF_centered, sigma_vals, mu_vals, A_vals, fallback_flags\n",
    "\n",
    "deltaF_center, sigma_vals, mu_vals, A_vals, fallback_flags = estimate_and_center_noise_model(\n",
    "    deltaF_F\n",
    ")"
   ],
   "id": "14158a71c60712ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume deltaF_F is (T, N), we need (N, T) for PCA\n",
    "data = deltaF_center.T  # shape: (neurons, time)\n",
    "pca = PCA(n_components=1)\n",
    "scores = pca.fit_transform(data)\n",
    "\n",
    "#Sort neurons by their score along the first PC\n",
    "sort_idx = np.argsort(scores[:, 0])\n",
    "sorted_data = deltaF_F[:, sort_idx].T  # shape: (neurons, time)\n",
    "\n",
    "# Compute time vector in seconds\n",
    "n_timepoints = sorted_data.shape[1]\n",
    "time = np.arange(n_timepoints) / fps  # time axis in seconds\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "im = plt.imshow(\n",
    "    sorted_data,\n",
    "    aspect='auto',\n",
    "    cmap='gray_r',\n",
    "    vmin=0, vmax=1,\n",
    "    extent=[time[0], time[-1], 0, sorted_data.shape[0]]\n",
    ")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"# Neuron (sorted)\")\n",
    "cbar = plt.colorbar(im, label=\"ΔF/F\")\n",
    "plt.title(\"ΔF/F Raster Plot (sorted by activity)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "aff3047e0a583da1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from skimage.measure import label, regionprops\n",
    "\n",
    "def normalize_dff(deltaF_center, sigma_vals):\n",
    "    \"\"\"\n",
    "    Normalize ΔF/F traces by dividing each neuron trace by its estimated noise level (σ).\n",
    "\n",
    "    Args:\n",
    "        deltaF_center (ndarray): ΔF/F matrix, shape (T, N)\n",
    "        sigma_vals (ndarray): Noise levels, shape (N,)\n",
    "\n",
    "    Returns:\n",
    "        ndarray: Normalized ΔF/F matrix\n",
    "    \"\"\"\n",
    "    return deltaF_center / sigma_vals\n",
    "\n",
    "def extract_transition_points(norm_data):\n",
    "    \"\"\"\n",
    "    Create a 2D point cloud of transitions between ΔF/F(t) and ΔF/F(t+1) for all neurons.\n",
    "\n",
    "    Args:\n",
    "        norm_data (ndarray): Normalized ΔF/F, shape (T, N)\n",
    "\n",
    "    Returns:\n",
    "        ndarray: 2D points of transitions, shape (~T*N, 2)\n",
    "    \"\"\"\n",
    "    x = norm_data[:-1, :].flatten()\n",
    "    y = norm_data[1:, :].flatten()\n",
    "    valid = ~np.isnan(x) & ~np.isnan(y)\n",
    "    return np.vstack((x[valid], y[valid])).T\n",
    "\n",
    "def estimate_kde_peak(points):\n",
    "    \"\"\"\n",
    "    Estimate the peak (mode) of ΔF/F(t) and ΔF/F(t+1) using KDE.\n",
    "\n",
    "    Args:\n",
    "        points (ndarray): 2D transitions, shape (N, 2)\n",
    "\n",
    "    Returns:\n",
    "        tuple: (peak_x, peak_y)\n",
    "    \"\"\"\n",
    "    kde_x = gaussian_kde(points[:, 0])\n",
    "    kde_y = gaussian_kde(points[:, 1])\n",
    "    xi = np.linspace(np.min(points), np.max(points), 200)\n",
    "    peak_x = xi[np.argmax(kde_x(xi))]\n",
    "    peak_y = xi[np.argmax(kde_y(xi))]\n",
    "    return peak_x, peak_y\n",
    "\n",
    "def generate_synthetic_noise(points, peak_x, peak_y):\n",
    "    \"\"\"\n",
    "    Create synthetic noise points from the lower-left (negative) quadrant of the data,\n",
    "    using a multivariate Gaussian with matching mean and covariance.\n",
    "\n",
    "    Args:\n",
    "        points (ndarray): Original transition points, shape (N, 2)\n",
    "        peak_x (float): Estimated mode of ΔF/F(t)\n",
    "        peak_y (float): Estimated mode of ΔF/F(t+1)\n",
    "\n",
    "    Returns:\n",
    "        ndarray: Synthetic noise points, shape (M, 2)\n",
    "    \"\"\"\n",
    "    points_neg = points[(points[:, 0] < peak_x) & (points[:, 1] < peak_y)]\n",
    "    mu_noise = [peak_x, peak_y]\n",
    "    if len(points_neg) < 2:\n",
    "        raise ValueError(\"Not enough points to compute covariance for noise model.\")\n",
    "    cov_noise = np.cov(points_neg, rowvar=False)\n",
    "    synthetic_noise = np.random.multivariate_normal(mu_noise, cov_noise, size=4 * len(points_neg))\n",
    "    return synthetic_noise\n",
    "\n",
    "def create_grid(points, synthetic_noise, n_bins):\n",
    "    \"\"\"\n",
    "    Create a shared 2D grid for histogramming data and synthetic noise.\n",
    "\n",
    "    Args:\n",
    "        points (ndarray): Real data points, shape (N, 2)\n",
    "        synthetic_noise (ndarray): Synthetic noise points, shape (M, 2)\n",
    "        n_bins (int): Number of bins for histogram in each dimension\n",
    "\n",
    "    Returns:\n",
    "        tuple: (grid, xev, yev)\n",
    "    \"\"\"\n",
    "    combined = np.concatenate([points.flatten(), synthetic_noise.flatten()])\n",
    "    vmin, vmax = np.floor(np.min(combined)), np.ceil(np.max(combined))\n",
    "    grid = np.linspace(vmin, vmax, n_bins)\n",
    "    xev, yev = np.meshgrid(grid, grid)\n",
    "    return grid, xev, yev\n",
    "\n",
    "\n",
    "def histogram2d(points, bins, grid):\n",
    "    \"\"\"\n",
    "    Create a 2D histogram of transition points based on a shared grid.\n",
    "\n",
    "    Args:\n",
    "        points (ndarray): Points to bin, shape (N, 2)\n",
    "        bins (int): Number of bins\n",
    "        grid (ndarray): Bin edges\n",
    "\n",
    "    Returns:\n",
    "        ndarray: 2D histogram, shape (bins, bins)\n",
    "    \"\"\"\n",
    "    xi = np.searchsorted(grid, points[:, 0]) - 1\n",
    "    yi = np.searchsorted(grid, points[:, 1]) - 1\n",
    "    xi = np.clip(xi, 0, bins - 1)\n",
    "    yi = np.clip(yi, 0, bins - 1)\n",
    "    hist = np.zeros((bins, bins))\n",
    "    for i in range(len(xi)):\n",
    "        hist[yi[i], xi[i]] += 1\n",
    "    return hist\n",
    "\n",
    "def compute_global_sigma(points, xev, yev, k_neighbors):\n",
    "    \"\"\"\n",
    "    Estimate the global Gaussian smoothing sigma by computing average distances\n",
    "    to the k-nearest real data points from each histogram bin location.\n",
    "\n",
    "    Args:\n",
    "        points (ndarray): Real data transition points, shape (N, 2)\n",
    "        xev (ndarray): Meshgrid x-coordinates\n",
    "        yev (ndarray): Meshgrid y-coordinates\n",
    "        k_neighbors (int): Number of neighbors for density estimation\n",
    "\n",
    "    Returns:\n",
    "        float: Estimated sigma for Gaussian filter\n",
    "    \"\"\"\n",
    "    grid_points = np.column_stack([xev.ravel(), yev.ravel()])\n",
    "    nn = NearestNeighbors(n_neighbors=k_neighbors)\n",
    "    nn.fit(points)\n",
    "    dists = nn.kneighbors(grid_points, return_distance=True)[0]\n",
    "    bin_spread = dists.mean(axis=1).reshape(xev.shape)\n",
    "    return np.median(bin_spread)\n",
    "\n",
    "def plot_density_comparison(density_data, density_noise, xev, yev, show=True):\n",
    "    \"\"\"\n",
    "    Plot side-by-side heatmaps of real vs synthetic noise densities.\n",
    "\n",
    "    Args:\n",
    "        density_data (ndarray): Smoothed density of real ΔF/F transitions\n",
    "        density_noise (ndarray): Smoothed density of synthetic noise\n",
    "        xev (ndarray): Meshgrid x-coordinates\n",
    "        yev (ndarray): Meshgrid y-coordinates\n",
    "        show (bool): Whether to display the plots\n",
    "    \"\"\"\n",
    "    if not show:\n",
    "        return\n",
    "\n",
    "    extent = [xev.min(), xev.max(), yev.min(), yev.max()]\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    im1 = axs[0].imshow(density_data.T, origin='lower', extent=extent, cmap='viridis', aspect='auto')\n",
    "    axs[0].set_title(\"Real ΔF/F Transitions Density\")\n",
    "    axs[0].set_xlabel(\"ΔF/F(t)\")\n",
    "    axs[0].set_ylabel(\"ΔF/F(t+1)\")\n",
    "    plt.colorbar(im1, ax=axs[0], label='Density')\n",
    "\n",
    "    im2 = axs[1].imshow(density_noise.T, origin='lower', extent=extent, cmap='viridis', aspect='auto')\n",
    "    axs[1].set_title(\"Synthetic Noise Density\")\n",
    "    axs[1].set_xlabel(\"ΔF/F(t)\")\n",
    "    axs[1].set_ylabel(\"ΔF/F(t+1)\")\n",
    "    plt.colorbar(im2, ax=axs[1], label='Density')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def compute_significant_odds(points, density_data, density_noise, xev, yev=None, confCutOff=95, plotFlag=False):\n",
    "    \"\"\"\n",
    "    Compute the significance mask where real transition density exceeds noise density by a confidence threshold.\n",
    "\n",
    "    Args:\n",
    "        points (ndarray): 2D transition points (ΔF/F(t), ΔF/F(t+1)), shape (M, 2)\n",
    "        density_data (ndarray): Smoothed histogram of real transitions\n",
    "        density_noise (ndarray): Smoothed histogram of synthetic noise transitions\n",
    "        xev (ndarray): Meshgrid X-coordinates\n",
    "        yev (ndarray, optional): Meshgrid Y-coordinates (for correct plotting)\n",
    "        confCutOff (float): Confidence level for thresholding (default 95)\n",
    "        plotFlag (bool): Whether to plot the significance map\n",
    "\n",
    "    Returns:\n",
    "        mapOfOdds (ndarray): Boolean mask (same shape as density_data) indicating significant bins\n",
    "    \"\"\"\n",
    "    pCutOff = (100 - confCutOff) / 100.0\n",
    "    mapOfOdds = density_noise <= (pCutOff * density_data)\n",
    "\n",
    "    if plotFlag:\n",
    "        if yev is None:\n",
    "            yev = xev  # fallback if symmetric grid\n",
    "\n",
    "        extent = [xev.min(), xev.max(), yev.min(), yev.max()]\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.plot(points[:, 0], points[:, 1], 'k.', alpha=0.2, markersize=1)\n",
    "        plt.imshow(mapOfOdds.T, extent=extent, origin='lower', aspect='auto', alpha=0.6, cmap='Reds')\n",
    "        plt.xlabel('z-transformed ΔF/F @ t', fontsize=14)\n",
    "        plt.ylabel('z-transformed ΔF/F @ t+1', fontsize=14)\n",
    "        plt.title(f'Significant Odds Mask (>{confCutOff}% confidence)')\n",
    "        plt.axis('equal')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return mapOfOdds\n",
    "\n",
    "def rasterize_with_odds(norm_data, mapOfOdds, xev, yev, fps, tauDecay):\n",
    "    \"\"\"\n",
    "    Rasterize ΔF/F activity using statistically significant transition maps,\n",
    "    adapted from Romano et al. (2017).\n",
    "\n",
    "    This function identifies significant neural transients by intersecting\n",
    "    statistical significance (mapOfOdds) with biologically plausible rise/decay\n",
    "    constraints, and uses 3-point temporal patterns to mark transients.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    norm_data : ndarray, shape (T, N)\n",
    "        Z-scored ΔF/F traces (i.e., ΔF/F divided by per-neuron sigma).\n",
    "    mapOfOdds : ndarray of bool, shape (B, B)\n",
    "        Boolean matrix indicating statistically significant transitions from ΔF/F(t) to ΔF/F(t+1).\n",
    "    xev, yev : ndarray, shape (B, B)\n",
    "        Meshgrids used for binning ΔF/F(t) and ΔF/F(t+1) values.\n",
    "    fps : float\n",
    "        Sampling rate (frames per second).\n",
    "    tauDecay : float\n",
    "        Expected calcium decay time constant (in seconds).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    raster : ndarray of int, shape (T, N)\n",
    "        Binary matrix indicating significant neural transients (1 = event).\n",
    "    mapOfOddsJoint : ndarray of bool, shape (B, B)\n",
    "        Refined joint transition map after applying baseline, decay, and rise filters.\n",
    "    \"\"\"\n",
    "    T, N = norm_data.shape\n",
    "    transfDataMatrix = norm_data\n",
    "\n",
    "    # --- Step 1: Remove baseline noise blob (connected to 0,0) ---\n",
    "    mask = ~mapOfOdds  # noise regions\n",
    "    labeled = label(mask)  # label connected regions\n",
    "    props = regionprops(labeled)\n",
    "    indZero = np.argmax(xev[0, :] > 0)  # bin closest to ΔF/F = 0\n",
    "\n",
    "    region_index = None\n",
    "    for i, region in enumerate(props):\n",
    "        if any(p[1] == indZero for p in region.coords):\n",
    "            region_index = region.label\n",
    "            break\n",
    "    # Create corrected map: keep all transitions except baseline blob\n",
    "    mapOfOddsCorrected = np.ones_like(mapOfOdds, dtype=bool)\n",
    "    if region_index is not None:\n",
    "        mapOfOddsCorrected[labeled == region_index] = 0\n",
    "\n",
    "    # Step 2: Build decay map — exclude transitions faster than biologically plausible decay\n",
    "    noiseBias = 1.0  # controls strictness of decay exclusion\n",
    "    factorDecay = np.exp(-1 / (fps * tauDecay))  # expected decay factor per frame\n",
    "    decayMap = np.ones_like(mapOfOdds, dtype=bool)\n",
    "    for col in range(mapOfOdds.shape[1]):\n",
    "        # Remove transitions where ΔF/F(t+1) decays too steeply from ΔF/F(t)\n",
    "        mask = yev[:, 0] < factorDecay * (xev[0, col] - noiseBias) - noiseBias\n",
    "        decayMap[mask, col] = 0\n",
    "\n",
    "   # Step 3: Build rise map — remove extreme up transitions at top of the grid\n",
    "    riseMap = np.ones_like(mapOfOdds, dtype=bool)\n",
    "    riseMap[-1, :] = 0  # Discard transitions ending at the highest ΔF/F(t+1) bin\n",
    "\n",
    "    # Step 4: Final joint significance map\n",
    "    mapOfOddsJoint = mapOfOddsCorrected & decayMap & riseMap\n",
    "\n",
    "    # Step 5: Raster generation\n",
    "    raster = np.zeros_like(transfDataMatrix, dtype=int)\n",
    "    bin_edges = xev[0, :]  # bin values used to digitize ΔF/F traces\n",
    "\n",
    "    # For each neuron\n",
    "    for n in range(N):\n",
    "        trace = transfDataMatrix[:, n]\n",
    "        bins = np.digitize(trace, bin_edges) - 1  # Convert ΔF/F values to bin indices\n",
    "\n",
    "        for t in range(2, T - 2):  # Leave padding for 2-point transitions\n",
    "            try:\n",
    "                # Test three 3-point transition patterns\n",
    "                optA = mapOfOddsJoint[bins[t + 1], bins[t]] and mapOfOddsJoint[bins[t], bins[t - 1]]\n",
    "                optB = mapOfOddsJoint[bins[t], bins[t - 1]] and mapOfOddsJoint[bins[t - 1], bins[t - 2]]\n",
    "                optC = mapOfOddsJoint[bins[t + 2], bins[t + 1]] and mapOfOddsJoint[bins[t + 1], bins[t]]\n",
    "                if optA or optB or optC:\n",
    "                    raster[t, n] = 1  # Mark as event\n",
    "            except IndexError:\n",
    "                continue  # Skip if bin index out of bounds\n",
    "\n",
    "    return raster, mapOfOddsJoint\n"
   ],
   "id": "1e2185664c81a07e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def compute_noise_model_romano_fast_modular(\n",
    "    deltaF_center,\n",
    "    sigma_vals,\n",
    "    n_bins=1000,\n",
    "    k_neighbors=100,\n",
    "    confCutOff=95,\n",
    "    plot_odds=False,\n",
    "    plot_density=False,\n",
    "    fps=2.0,\n",
    "    tauDecay=3.0,\n",
    "\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute the Romano-style noise model and generate a raster of significant transients.\n",
    "\n",
    "    This pipeline analyzes ΔF/F transitions using statistical and biophysical constraints\n",
    "    to distinguish real neural events from noise.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    deltaF_center : ndarray of shape (T, N)\n",
    "        ΔF/F traces across time (T) and neurons (N).\n",
    "    sigma_vals : ndarray of shape (N,)\n",
    "        Estimated noise standard deviation per neuron.\n",
    "    n_bins : int\n",
    "        Number of bins for KDE/histogram grid.\n",
    "    k_neighbors : int\n",
    "        Number of nearest neighbors used to estimate smoothing scale.\n",
    "    confCutOff : float\n",
    "        Confidence level (in percent) used to threshold real vs noise transitions.\n",
    "    plot_odds : bool\n",
    "        If True, show significant transition mask.\n",
    "    plot_density : bool\n",
    "        If True, plot transition density of real vs synthetic noise.\n",
    "    fps : float\n",
    "        Imaging frame rate (Hz).\n",
    "    tauDecay : float\n",
    "        Calcium decay time constant (in seconds).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mapOfOdds : ndarray of shape (B, B)\n",
    "        Binary map of significant transitions (real > noise).\n",
    "    density_data : ndarray\n",
    "        Smoothed density of real ΔF/F(t) → ΔF/F(t+1) transitions.\n",
    "    density_noise : ndarray\n",
    "        Smoothed density from synthetic Gaussian noise model.\n",
    "    xev, yev : ndarray\n",
    "        Meshgrid coordinates used for binning.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Step 1: Normalize ΔF/F using per-neuron σ values\n",
    "    norm_data = normalize_dff(deltaF_center, sigma_vals)\n",
    "\n",
    "    # --- Step 2: Extract point cloud of ΔF/F(t), ΔF/F(t+1) transitions\n",
    "    points = extract_transition_points(norm_data)\n",
    "\n",
    "    # --- Step 3: Estimate the peak of KDE in each dimension to locate mode\n",
    "    peak_x, peak_y = estimate_kde_peak(points)\n",
    "\n",
    "    # --- Step 4: Generate synthetic noise from the negative quadrant\n",
    "    # Modeled as a multivariate Gaussian fit to low activity region\n",
    "    synthetic_noise = generate_synthetic_noise(points, peak_x, peak_y)\n",
    "\n",
    "    # --- Step 5: Create a common 2D grid for histogramming real and noise data\n",
    "    grid, xev, yev = create_grid(points, synthetic_noise, n_bins)\n",
    "\n",
    "    # --- Step 6: Bin the real data and synthetic noise into histograms\n",
    "    hist_data = histogram2d(points, n_bins, grid)\n",
    "    hist_noise = histogram2d(synthetic_noise, n_bins, grid)\n",
    "\n",
    "    # --- Step 7: Estimate adaptive smoothing scales via KNN on each distribution\n",
    "    sigma_data = compute_global_sigma(points, xev, yev, k_neighbors)\n",
    "    sigma_noise = compute_global_sigma(synthetic_noise, xev, yev, k_neighbors)\n",
    "\n",
    "    # --- Step 8: Apply Gaussian smoothing to both histograms\n",
    "    density_data = gaussian_filter(hist_data, sigma=sigma_data)\n",
    "    density_noise = gaussian_filter(hist_noise, sigma=sigma_noise)\n",
    "\n",
    "    # --- Step 9: Optionally show real vs noise density comparison\n",
    "    if plot_density:\n",
    "        plot_density_comparison(density_data, density_noise, xev, yev, show=True)\n",
    "\n",
    "    # --- Step 10: Build a binary significance map (real density > noise threshold)\n",
    "    mapOfOdds = compute_significant_odds(\n",
    "        points,\n",
    "        density_data,\n",
    "        density_noise,\n",
    "        xev,\n",
    "        yev,\n",
    "        confCutOff=confCutOff,\n",
    "        plotFlag=plot_odds\n",
    "    )\n",
    "\n",
    "    # --- Step 11: Apply Romano rasterization with decay + rise constraints\n",
    "    raster, mapOfOddsJoint = rasterize_with_odds(\n",
    "        norm_data=norm_data,\n",
    "        mapOfOdds=mapOfOdds,\n",
    "        xev=xev,\n",
    "        yev=yev,\n",
    "        fps=fps,\n",
    "        tauDecay=tauDecay\n",
    "    )\n",
    "\n",
    "    return mapOfOdds, density_data, density_noise, xev, yev, raster, mapOfOddsJoint\n",
    "\n",
    "\n",
    "mapOfOdds, density_data, density_noise, xev, yev, raster, mapOfOddsJoint = compute_noise_model_romano_fast_modular(\n",
    "    deltaF_center,\n",
    "    sigma_vals,\n",
    "    n_bins=1000,\n",
    "    k_neighbors=100,\n",
    "    confCutOff=95,\n",
    "    plot_odds=False,\n",
    "    plot_density=False,\n",
    "    fps = 2.0,       # or your actual imaging rate\n",
    "    tauDecay = 3.0,   # seconds (depends on calcium indicator, e.g. GCaMP6s ~ 1s)\n",
    ")\n"
   ],
   "id": "1473795fd13e2e05",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_raster_sorted_by_pca(raster_clean, fps=2):\n",
    "    \"\"\"\n",
    "    Plots a binary raster (0/1) sorted by PCA on neural activity.\n",
    "\n",
    "    Parameters:\n",
    "        raster: (T, N) binary matrix of significant events\n",
    "        fps: frames per second\n",
    "    \"\"\"\n",
    "    data = raster_clean.T  # (neurons, time)\n",
    "    #pca = PCA(n_components=1)\n",
    "    #scores = pca.fit_transform(data)\n",
    "\n",
    "    #sort_idx = np.argsort(scores[:, 0])\n",
    "    sorted_raster = raster_clean[:, sort_idx].T  # (neurons, time)\n",
    "\n",
    "    time = np.arange(sorted_raster.shape[1]) / fps\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    im = plt.imshow(\n",
    "        sorted_raster,\n",
    "        aspect='auto',\n",
    "        cmap='Greys',\n",
    "        vmin=0, vmax=1,\n",
    "        extent=[time[0], time[-1], 0, sorted_raster.shape[0]]\n",
    "    )\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"# Neuron (PCA sorted)\")\n",
    "    cbar = plt.colorbar(im, label=\"Activity (0/1)\")\n",
    "    plt.title(\"Raster Plot (neurons sorted by PCA)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return sort_idx\n",
    "\n",
    "sort_idx = plot_raster_sorted_by_pca(raster, fps=2)\n"
   ],
   "id": "62176f0fe5149bb6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "c43b63968a9f22b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Assume deltaF_F is (T, N), we need (N, T) for PCA\n",
    "data = deltaF_center.T  # shape: (neurons, time)\n",
    "#pca = PCA(n_components=1)\n",
    "#scores = pca.fit_transform(data)\n",
    "\n",
    "#Sort neurons by their score along the first PC\n",
    "#sort_idx = np.argsort(scores[:, 0])\n",
    "sorted_data = deltaF_F[:, sort_idx].T  # shape: (neurons, time)\n",
    "\n",
    "# Compute time vector in seconds\n",
    "n_timepoints = sorted_data.shape[1]\n",
    "time = np.arange(n_timepoints) / fps  # time axis in seconds\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "im = plt.imshow(\n",
    "    sorted_data,\n",
    "    aspect='auto',\n",
    "    cmap='gray_r',\n",
    "    vmin=0, vmax=0.3,\n",
    "    extent=[time[0], time[-1], 0, sorted_data.shape[0]]\n",
    ")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"# Neuron (sorted)\")\n",
    "cbar = plt.colorbar(im, label=\"ΔF/F\")\n",
    "plt.title(\"ΔF/F Raster Plot (sorted by activity)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "f55c51af65728222",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
