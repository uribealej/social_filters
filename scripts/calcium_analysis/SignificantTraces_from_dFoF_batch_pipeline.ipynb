{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a57161a2",
   "metadata": {},
   "source": [
    "\n",
    "# Significant Traces — Batch Pipeline (reads existing dFoF)\n",
    "\n",
    "Scans experiments (e.g., `L433_f02_Exp_1_flickering`) under your base path, looks in each\n",
    "`03_analysis/functional/suite2P/plane*/dFoF/` folder for inputs, computes significant traces using\n",
    "`src/significant_traces.py`, saves arrays & a plot into the same folder, and updates `metadata.json`.\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T15:27:50.654299Z",
     "start_time": "2025-10-15T15:27:50.650343Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# ==== Configuration ============================================================\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path(r\"C:\\Users\\suribear\\OneDrive - Université de Lausanne\\Lab\\Data\\2p\")\n",
    "EXPERIMENT_WHITELIST = ['L433_f02_Exp_1_flickering']\n",
    "\n",
    "FPS_DEFAULT = 2.0\n",
    "TAUDECAY_DEFAULT = 6.0\n",
    "\n",
    "ST_N_BINS       = 1000\n",
    "ST_K_NEIGHBORS  = 100\n",
    "ST_CONF_CUTOFF  = 90\n",
    "ST_PLOT_ODDS    = False\n",
    "ST_VMAX_DFF     = 0.40\n",
    "\n",
    "SRC_DIR = Path.cwd() / \"src\"\n",
    "SHOW_PLOTS_INLINE = True\n",
    "WRITE_SESSION_SUMMARY = True\n"
   ],
   "id": "bb0e625323b737bb",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T15:28:44.056623Z",
     "start_time": "2025-10-15T15:28:44.033381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# ==== Imports & Helpers ========================================================\n",
    "import sys, json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.append(str(SRC_DIR))\n",
    "\n",
    "from src.significant_traces import compute_noise_model_romano_fast_modular, plot_dff_and_raster\n",
    "\n",
    "def ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def find_experiments(base_dir: Path, whitelist=None):\n",
    "    out = []\n",
    "    for child in base_dir.iterdir():\n",
    "        if not child.is_dir():\n",
    "            continue\n",
    "        if whitelist and child.name not in whitelist:\n",
    "            continue\n",
    "        s2p = child / \"03_analysis\" / \"functional\" / \"suite2P\"\n",
    "        if s2p.exists() and any(p.is_dir() and p.name.startswith(\"plane\") for p in s2p.iterdir()):\n",
    "            out.append(child)\n",
    "    return sorted(out, key=lambda p: p.name)\n",
    "\n",
    "def plane_dirs(exp_dir: Path):\n",
    "    s2p = exp_dir / \"03_analysis\" / \"functional\" / \"suite2P\"\n",
    "    return sorted([p for p in s2p.iterdir() if p.is_dir() and p.name.startswith(\"plane\")], key=lambda p: p.name)\n",
    "\n",
    "def experiment_prefix(exp_name: str) -> str:\n",
    "    parts = exp_name.split(\"_\")\n",
    "    if len(parts) >= 2:\n",
    "        return parts[0] + \"_\" + parts[1]\n",
    "    return exp_name\n",
    "\n",
    "def read_params_from_metadata(dfof_dir: Path, fps_default: float, tauDecay_default: float):\n",
    "    meta_path = dfof_dir / \"metadata.json\"\n",
    "    fps = fps_default\n",
    "    tauDecay = tauDecay_default\n",
    "    if meta_path.exists():\n",
    "        try:\n",
    "            with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                meta = json.load(f)\n",
    "            fps = float(meta.get(\"fps\", fps))\n",
    "            tauDecay = float(meta.get(\"tauDecay\", meta.get(\"tau\", tauDecay)))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return fps, tauDecay\n",
    "\n",
    "def update_npz(npz_path: Path, additions: dict):\n",
    "    payload = {}\n",
    "    if npz_path.exists():\n",
    "        try:\n",
    "            with np.load(npz_path, allow_pickle=False) as z:\n",
    "                for k in z.files:\n",
    "                    payload[k] = z[k]\n",
    "        except Exception:\n",
    "            payload = {}\n",
    "    payload.update(additions)\n",
    "    np.savez_compressed(npz_path, **payload)\n",
    "\n",
    "def update_metadata(meta_path: Path, st_params: dict, filenames: dict, shapes: dict):\n",
    "    try:\n",
    "        meta = {}\n",
    "        if meta_path.exists():\n",
    "            with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                meta = json.load(f)\n",
    "        st_block = meta.get(\"significant_traces\", {})\n",
    "        st_block.update({\n",
    "            \"params\": st_params,\n",
    "            \"filenames\": filenames,\n",
    "            \"shapes\": shapes,\n",
    "            \"updated_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "        })\n",
    "        meta[\"significant_traces\"] = st_block\n",
    "        with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(meta, f, indent=2)\n",
    "    except Exception as e:\n",
    "        print(\"WARNING: could not update metadata:\", e)\n"
   ],
   "id": "19bf64f9049afede",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# ==== Batch Run ================================================================\n",
    "import pandas as pd\n",
    "\n",
    "session_ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "session_root = BASE_DIR / f\"session_significant_{session_ts}\"\n",
    "if WRITE_SESSION_SUMMARY:\n",
    "    ensure_dir(session_root)\n",
    "\n",
    "experiments = find_experiments(BASE_DIR, whitelist=EXPERIMENT_WHITELIST)\n",
    "print(f\"Experiments found: {len(experiments)}\")\n",
    "\n",
    "summary_rows = []\n",
    "\n",
    "for exp_dir in experiments:\n",
    "    exp_name   = exp_dir.name\n",
    "    prefix     = experiment_prefix(exp_name)\n",
    "    planes     = plane_dirs(exp_dir)\n",
    "    print(f\"- {exp_name}: {len(planes)} plane(s)\")\n",
    "\n",
    "    for pdir in planes:\n",
    "        dfof_dir = pdir / \"dFoF\"\n",
    "        if not dfof_dir.exists():\n",
    "            continue\n",
    "\n",
    "        fps, tauDecay = read_params_from_metadata(dfof_dir, FPS_DEFAULT, TAUDECAY_DEFAULT)\n",
    "\n",
    "        dFoF_path_candidates = [\n",
    "            dfof_dir / f\"{prefix}_dFoF.npy\",\n",
    "            dfof_dir / f\"dFoF_{prefix}.npy\"\n",
    "        ]\n",
    "        dFoF_path = next((p for p in dFoF_path_candidates if p.exists()), None)\n",
    "        if dFoF_path is None:\n",
    "            continue\n",
    "\n",
    "        dFoF = np.load(dFoF_path)\n",
    "\n",
    "        (mapOfOdds,\n",
    "         deltaF_center,\n",
    "         density_data,\n",
    "         density_noise,\n",
    "         xev, yev,\n",
    "         raster,\n",
    "         mapOfOddsJoint) = compute_noise_model_romano_fast_modular(\n",
    "             dFoF,\n",
    "             n_bins=ST_N_BINS,\n",
    "             k_neighbors=ST_K_NEIGHBORS,\n",
    "             confCutOff=ST_CONF_CUTOFF,\n",
    "             plot_odds=ST_PLOT_ODDS,\n",
    "             fps=fps,\n",
    "             tauDecay=tauDecay,\n",
    "        )\n",
    "\n",
    "        raster_out        = dfof_dir / f\"{prefix}_significant_traces.npy\"\n",
    "        deltaF_center_out = dfof_dir / f\"{prefix}_dFoF_center.npy\"\n",
    "        np.save(raster_out, raster)\n",
    "        np.save(deltaF_center_out, deltaF_center)\n",
    "\n",
    "        npz_path = dfof_dir / f\"{prefix}_dFoF_outputs.npz\"\n",
    "        update_npz(npz_path, {\"deltaF_center\": deltaF_center, \"raster\": raster})\n",
    "\n",
    "        sort_idx = plot_dff_and_raster(deltaF_center, raster, fps=fps, vmax_dff=ST_VMAX_DFF)\n",
    "        fig = plt.gcf()\n",
    "        plot_out = dfof_dir / f\"{prefix}_significant_traces_plot.png\"\n",
    "        fig.savefig(plot_out, dpi=150, bbox_inches=\"tight\")\n",
    "        if SHOW_PLOTS_INLINE and pdir == planes[-1]:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close(fig)\n",
    "\n",
    "        meta_path = dfof_dir / \"metadata.json\"\n",
    "        st_params = dict(n_bins=ST_N_BINS, k_neighbors=ST_K_NEIGHBORS, confCutOff=ST_CONF_CUTOFF, fps=fps, tauDecay=tauDecay)\n",
    "        filenames = dict(raster_npy=raster_out.name, dFoF_center_npy=deltaF_center_out.name, plot_png=plot_out.name, npz_bundle=npz_path.name)\n",
    "        shapes = dict(deltaF_center=list(deltaF_center.shape), raster=list(raster.shape))\n",
    "        update_metadata(meta_path, st_params, filenames, shapes)\n",
    "\n",
    "        summary_rows.append({\"experiment\": exp_name, \"plane\": pdir.name, \"fps\": fps, \"tauDecay\": tauDecay,\n",
    "                             \"raster_npy\": raster_out.name, \"dFoF_center_npy\": deltaF_center_out.name, \"plot_png\": plot_out.name})\n",
    "\n",
    "if WRITE_SESSION_SUMMARY and summary_rows:\n",
    "    df = pd.DataFrame(summary_rows).sort_values([\"experiment\", \"plane\"])\n",
    "    df_path = session_root / \"summary.csv\"\n",
    "    df.to_csv(df_path, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Session summary saved to: {df_path}\")\n",
    "else:\n",
    "    print(\"No outputs written (check inputs or whitelist).\")\n"
   ],
   "id": "ea099e233f2ad376"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T15:38:26.387653Z",
     "start_time": "2025-10-15T15:38:26.379037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# (Optional) Preview latest session summary\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "try:\n",
    "    from IPython.display import display\n",
    "except Exception:\n",
    "    display = None\n",
    "\n",
    "sessions = sorted([p for p in BASE_DIR.iterdir() if p.is_dir() and p.name.startswith(\"session_significant_\")])\n",
    "if sessions:\n",
    "    latest = sessions[-1]\n",
    "    csv_path = latest / \"summary.csv\"\n",
    "    if csv_path.exists():\n",
    "        df = pd.read_csv(csv_path)\n",
    "        if display:\n",
    "            display(df)\n",
    "        else:\n",
    "            print(df.to_string(index=False))\n",
    "        print(f\"Displayed summary from: {csv_path}\")\n",
    "    else:\n",
    "        print(f\"No summary.csv in {latest}\")\n",
    "else:\n",
    "    print(\"No session_significant_* folder found yet.\")\n"
   ],
   "id": "e740a4cb194b981d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No session_significant_* folder found yet.\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
