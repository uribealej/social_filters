{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3da8de02",
   "metadata": {},
   "source": [
    "\n",
    "# dFoF Batch Extraction (Suite2p → dFoF → Save)\n",
    "\n",
    "This notebook scans your experiment directories (each containing `03_analysis/functional/suite2P/plane*`),\n",
    "loads Suite2p outputs, runs **`process_suite2p_fluorescence`** from your **`dff_extraction`** repo, and saves outputs per-plane\n",
    "in a new **`dFoF/`** subfolder. It also captures printed messages into per-plane log files and a session log.\n",
    "\n",
    "Also merge per-plane dFoF outputs into a single experiment-level file in a later cell.\n",
    "Also you can validate the merged outputs in the last cell.\n",
    "the plotted raster image helps visually validate the merged result as well, this is save in the analysis folder.\n",
    "\n",
    "outputs are saved as `.npy` arrays and a compressed `.npz` bundle, along with a `metadata.json` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "fe7c4b22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T12:01:40.894624Z",
     "start_time": "2025-10-28T12:01:40.889742Z"
    }
   },
   "source": [
    "\n",
    "# ==== Configuration (edit this) ===============================================\n",
    "from pathlib import Path\n",
    "\n",
    "# Base directory containing your experiment folders (Windows path allowed)\n",
    "# Example: r\"C:\\Users\\suribear\\OneDrive - Université de Lausanne\\Lab\\Data\\2p\"\n",
    "BASE_DIR = Path(r\"C:\\Users\\suribear\\OneDrive - Université de Lausanne\\Lab\\Data\\2p\")\n",
    "\n",
    "# Optional: limit to specific experiment folders (by name) if you want\n",
    "# Leave empty to process all subfolders that contain suite2P/plane* structure.\n",
    "EXPERIMENT_WHITELIST = [\n",
    "    \"L433_f02_Exp_1_flickering\",\n",
    "    \"L433_f03_Exp_1_flickering\",\n",
    "    \"L433_f04_Exp_1_flickering\",\n",
    "    \"L433_f05_Exp_1_flickering\",\n",
    "    \"L433_f06_Exp_1_flickering\",\n",
    "    \"L453_f07_Exp_1_flickering\",\n",
    "    \"L453_f08_Exp_1_flickering\",\n",
    "    \"L453_f09_Exp_1_flickering\",\n",
    "    \"L453_f10_Exp_1_flickering\",\n",
    "    \"L453_f11_Exp_1_flickering\",\n",
    "    \"L472_f01_Exp_2_rocking_1\",\n",
    "    \"L472_f02_Exp_2_rocking_1\",\n",
    "    \"L472_f03_Exp_2_rocking_1\",\n",
    "    \"L472_f04_Exp_2_rocking_1\",\n",
    "    \"L472_f05_Exp_2_rocking_1\",\n",
    "    \"L472_f06_Exp_2_rocking_1\",\n",
    "]  # e.g., [\"L433_f02_Exp_1_flickering\", \"L472_f03_Exp_2_rocking_1\"]\n",
    "\n",
    "# Imaging parameters (global defaults). Override per-experiment below if needed.\n",
    "FPS_DEFAULT = 2.0          # Hz\n",
    "TAU_DEFAULT = 6          # seconds\n",
    "\n",
    "# dF/F extraction params (tune as needed)\n",
    "PERCENTILE = 8\n",
    "INSTABILITY_RATIO = 0.1\n",
    "MIN_WINDOW_S = 15\n",
    "WINDOW_TAU_MULTIPLIER = 40\n",
    "MIN_STD = 0.003  # std gate used by your function filter_inactive_rois_by_std_or_z\n",
    "\n",
    "# Per-experiment overrides (optional). Keys must be experiment folder names.\n",
    "# Example:\n",
    "# PER_EXPERIMENT = {\n",
    "#     \"L433_f02_Exp_1_flickering\": {\"fps\": 3.0, \"tau\": 0.7},\n",
    "#     \"L472_f03_Exp_2_rocking_1\": {\"fps\": 4.0, \"tau\": 0.9},\n",
    "# }\n",
    "PER_EXPERIMENT = {}\n",
    "\n",
    "print(\"Configured. Edit the values above as needed and run the next cell to proceed.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured. Edit the values above as needed and run the next cell to proceed.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "5b035a01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T12:01:52.872447Z",
     "start_time": "2025-10-28T12:01:52.859789Z"
    }
   },
   "source": [
    "\n",
    "# ==== Imports & Helper Utilities ==============================================\n",
    "import sys, io, json, traceback\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure your repository (that contains dff_extraction.py) is on sys.path if needed.\n",
    "# Example (uncomment & edit): sys.path.append(r\"C:\\Users\\suribear\\code\\dff_extraction_repo\")\n",
    "try:\n",
    "    from src.dff_extraction import process_suite2p_fluorescence\n",
    "except Exception as e:\n",
    "    print(\"Could not import 'process_suite2p_fluorescence' from dff_extraction. \"\n",
    "          \"If your repo isn't on sys.path, append it below and re-run this cell.\")\n",
    "    # sys.path.append(r\"C:\\Users\\suribear\\code\\dff_extraction_repo\")\n",
    "    # from dff_extraction import process_suite2p_fluorescence\n",
    "    raise\n",
    "\n",
    "class TeeWriter:\n",
    "    def __init__(self, *streams):\n",
    "        self.streams = streams\n",
    "    def write(self, data):\n",
    "        for s in self.streams:\n",
    "            s.write(data)\n",
    "    def flush(self):\n",
    "        for s in self.streams:\n",
    "            s.flush()\n",
    "\n",
    "def ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def find_experiments(base_dir: Path, whitelist=None):\n",
    "    \"\"\"Return list of experiment directories that contain suite2P/plane* structure.\"\"\"\n",
    "    candidates = []\n",
    "    for child in base_dir.iterdir():\n",
    "        if not child.is_dir():\n",
    "            continue\n",
    "        if whitelist and child.name not in whitelist:\n",
    "            continue\n",
    "        s2p_root = child / \"03_analysis\" / \"functional\" / \"suite2P\"\n",
    "        if s2p_root.exists() and any(p.is_dir() and p.name.startswith(\"plane\") for p in s2p_root.iterdir()):\n",
    "            candidates.append(child)\n",
    "    return sorted(candidates, key=lambda p: p.name)\n",
    "\n",
    "def plane_dirs(exp_dir: Path):\n",
    "    s2p_root = exp_dir / \"03_analysis\" / \"functional\" / \"suite2P\"\n",
    "    return sorted([p for p in s2p_root.iterdir() if p.is_dir() and p.name.startswith(\"plane\")], key=lambda p: p.name)\n",
    "\n",
    "def experiment_prefix(exp_name: str) -> str:\n",
    "    \"\"\"Build prefix like 'L433_f02' from 'L433_f02_Exp_1_flickering'.\"\"\"\n",
    "    parts = exp_name.split(\"_\")\n",
    "    if len(parts) >= 2:\n",
    "        return parts[0] + \"_\" + parts[1]\n",
    "    return exp_name\n",
    "\n",
    "def save_outputs(out_dir: Path, prefix: str, dFoF: np.ndarray, filtered_roi_indices: np.ndarray, params: dict):\n",
    "    ensure_dir(out_dir)\n",
    "    # File paths\n",
    "    dfof_path = out_dir / f\"{prefix}_dFoF.npy\"\n",
    "    idx_path  = out_dir / f\"{prefix}_filtered_roi_indices.npy\"\n",
    "    npz_path  = out_dir / f\"{prefix}_dFoF_outputs.npz\"\n",
    "\n",
    "    # Save arrays\n",
    "    np.save(dfof_path, dFoF)\n",
    "    np.save(idx_path, filtered_roi_indices)\n",
    "    np.savez_compressed(npz_path, dFoF=dFoF, filtered_roi_indices=filtered_roi_indices)\n",
    "\n",
    "    # Metadata\n",
    "    meta = {\n",
    "        \"created_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "        \"shape_dFoF\": list(dFoF.shape),\n",
    "        \"n_final_rois\": int(dFoF.shape[1] if dFoF.ndim == 2 else 0),\n",
    "        \"file_prefix\": prefix,\n",
    "        \"out_folder\": str(out_dir),\n",
    "        \"fps\": params.get(\"fps\"),\n",
    "        \"tau\": params.get(\"tau\"),\n",
    "        \"percentile\": params.get(\"percentile\"),\n",
    "        \"instability_ratio\": params.get(\"instability_ratio\"),\n",
    "        \"min_window_s\": params.get(\"min_window_s\"),\n",
    "        \"window_tau_multiplier\": params.get(\"window_tau_multiplier\"),\n",
    "        \"min_std\": params.get(\"min_std\"),\n",
    "        \"filenames\": {\n",
    "            \"dFoF_npy\": dfof_path.name,\n",
    "            \"filtered_roi_indices_npy\": idx_path.name,\n",
    "            \"npz_bundle\": npz_path.name,\n",
    "        }\n",
    "    }\n",
    "    with open(out_dir / \"metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "\n",
    "print(\"Helpers loaded.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpers loaded.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "4a66beb1",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-10-27T16:33:56.202830Z"
    }
   },
   "source": [
    "\n",
    "# ==== Batch Processing (no plots) =============================================\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "session_ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "session_log_root = BASE_DIR / f\"session_logs_{session_ts}\"\n",
    "ensure_dir(session_log_root)\n",
    "\n",
    "experiments = find_experiments(BASE_DIR, whitelist=EXPERIMENT_WHITELIST)\n",
    "print(f\"Found {len(experiments)} experiment folder(s) in: {BASE_DIR}\")\n",
    "if experiments:\n",
    "    for e in experiments:\n",
    "        print(\" -\", e.name)\n",
    "\n",
    "results = []\n",
    "\n",
    "for exp_dir in experiments:\n",
    "    exp_name = exp_dir.name\n",
    "    exp_prefix = experiment_prefix(exp_name)\n",
    "    overrides = PER_EXPERIMENT.get(exp_name, {})\n",
    "    fps = float(overrides.get(\"fps\", FPS_DEFAULT))\n",
    "    tau = float(overrides.get(\"tau\", TAU_DEFAULT))\n",
    "\n",
    "    planes = plane_dirs(exp_dir)\n",
    "    print(f\"\\n=== {exp_name}: {len(planes)} plane(s) detected ===\")\n",
    "\n",
    "    for pdir in planes:\n",
    "        print(f\"\\nProcessing {pdir} ...\")\n",
    "        dfof_dir = pdir / \"dFoF\"   # NEW output folder name\n",
    "        ensure_dir(dfof_dir)\n",
    "\n",
    "        # Per-plane log\n",
    "        plane_log_path = dfof_dir / f\"log_{exp_name}_{pdir.name}_{session_ts}.txt\"\n",
    "        session_log_path = session_log_root / f\"log_{exp_name}_{pdir.name}_{session_ts}.txt\"\n",
    "\n",
    "        # Tee prints to both file and notebook output\n",
    "        with open(plane_log_path, \"w\", encoding=\"utf-8\") as flog, open(session_log_path, \"w\", encoding=\"utf-8\") as fsess:\n",
    "            tee = TeeWriter(sys.stdout, flog, fsess)\n",
    "            old_stdout = sys.stdout\n",
    "            try:\n",
    "                sys.stdout = tee\n",
    "\n",
    "                print(f\"[{datetime.now().isoformat(timespec='seconds')}] Starting plane {pdir.name}\")\n",
    "                print(f\"Using fps={fps}, tau={tau}\")\n",
    "                print(\"Calling process_suite2p_fluorescence...\")\n",
    "\n",
    "                dFoF, filtered_indices = process_suite2p_fluorescence(\n",
    "                    f_path=pdir,\n",
    "                    fps=fps,\n",
    "                    tau=tau,\n",
    "                    percentile=PERCENTILE,\n",
    "                    instability_ratio=INSTABILITY_RATIO,\n",
    "                    min_window_s=MIN_WINDOW_S,\n",
    "                    window_tau_multiplier=WINDOW_TAU_MULTIPLIER,\n",
    "                    min_std=MIN_STD,\n",
    "                )\n",
    "\n",
    "                if not isinstance(dFoF, np.ndarray) or dFoF.ndim != 2:\n",
    "                    raise ValueError(\"Expected dFoF to be a 2D numpy array (T x N).\")\n",
    "\n",
    "                if not isinstance(filtered_indices, (np.ndarray, list)):\n",
    "                    raise ValueError(\"Expected filtered_indices to be an array-like of ROI indices.\")\n",
    "                filtered_indices = np.array(filtered_indices)\n",
    "\n",
    "                print(\"Saving arrays and metadata...\")\n",
    "                params = dict(\n",
    "                    fps=fps, tau=tau,\n",
    "                    percentile=PERCENTILE,\n",
    "                    instability_ratio=INSTABILITY_RATIO,\n",
    "                    min_window_s=MIN_WINDOW_S,\n",
    "                    window_tau_multiplier=WINDOW_TAU_MULTIPLIER,\n",
    "                    min_std=MIN_STD,\n",
    "                )\n",
    "                save_outputs(dfof_dir, exp_prefix, dFoF, filtered_indices, params)\n",
    "\n",
    "                T, N = dFoF.shape\n",
    "                results.append({\n",
    "                    \"experiment\": exp_name,\n",
    "                    \"plane\": pdir.name,\n",
    "                    \"n_rois_final\": int(N),\n",
    "                    \"T\": int(T),\n",
    "                    \"fps\": fps,\n",
    "                    \"tau\": tau,\n",
    "                    \"dFoF_dir\": str(dfof_dir),\n",
    "                    \"file_prefix\": exp_prefix,\n",
    "                })\n",
    "\n",
    "                print(f\"Done: {pdir.name} -> N={N} ROIs, T={T}. Outputs in: {dfof_dir}\")\n",
    "\n",
    "            except Exception as ex:\n",
    "                print(\"ERROR while processing\", pdir)\n",
    "                traceback.print_exc()\n",
    "            finally:\n",
    "                sys.stdout = old_stdout\n",
    "\n",
    "# Summary table saved to the session log root\n",
    "if results:\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(results).sort_values([\"experiment\", \"plane\"])\n",
    "    df_path = session_log_root / \"summary.csv\"\n",
    "    df.to_csv(df_path, index=False, encoding=\"utf-8\")\n",
    "    print(f\"\\nSession summary saved to: {df_path}\")\n",
    "else:\n",
    "    print(\"\\nNo results to summarize. Check your BASE_DIR or whitelist settings.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16 experiment folder(s) in: C:\\Users\\suribear\\OneDrive - Université de Lausanne\\Lab\\Data\\2p\n",
      " - L433_f02_Exp_1_flickering\n",
      " - L433_f03_Exp_1_flickering\n",
      " - L433_f04_Exp_1_flickering\n",
      " - L433_f05_Exp_1_flickering\n",
      " - L433_f06_Exp_1_flickering\n",
      " - L453_f07_Exp_1_flickering\n",
      " - L453_f08_Exp_1_flickering\n",
      " - L453_f09_Exp_1_flickering\n",
      " - L453_f10_Exp_1_flickering\n",
      " - L453_f11_Exp_1_flickering\n",
      " - L472_f01_Exp_2_rocking_1\n",
      " - L472_f02_Exp_2_rocking_1\n",
      " - L472_f03_Exp_2_rocking_1\n",
      " - L472_f04_Exp_2_rocking_1\n",
      " - L472_f05_Exp_2_rocking_1\n",
      " - L472_f06_Exp_2_rocking_1\n",
      "\n",
      "=== L433_f02_Exp_1_flickering: 5 plane(s) detected ===\n",
      "\n",
      "Processing C:\\Users\\suribear\\OneDrive - Université de Lausanne\\Lab\\Data\\2p\\L433_f02_Exp_1_flickering\\03_analysis\\functional\\suite2P\\plane0 ...\n",
      "[2025-10-27T17:33:56] Starting plane plane0\n",
      "Using fps=2.0, tau=6.0\n",
      "Calling process_suite2p_fluorescence...\n",
      "Excluded 139 non-cell ROIs. Remaining: 449 cells.\n",
      "Removed 0 dim ROIs.\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f6faefe2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T12:01:58.352674Z",
     "start_time": "2025-10-28T12:01:58.323640Z"
    }
   },
   "source": [
    "# (Optional) Display summary table if available — no external helpers\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "try:\n",
    "    from IPython.display import display  # works in Jupyter\n",
    "except Exception:\n",
    "    display = None\n",
    "\n",
    "session_dirs = sorted([p for p in BASE_DIR.iterdir() if p.is_dir() and p.name.startswith(\"session_logs_\")])\n",
    "if session_dirs:\n",
    "    latest = session_dirs[-1]\n",
    "    summary_csv = latest / \"summary.csv\"\n",
    "    if summary_csv.exists():\n",
    "        df = pd.read_csv(summary_csv)\n",
    "        if display is not None:\n",
    "            display(df)\n",
    "        else:\n",
    "            print(df.to_string(index=False))\n",
    "        print(f\"Displayed summary from: {summary_csv}\")\n",
    "    else:\n",
    "        print(f\"No summary.csv in {latest}\")\n",
    "else:\n",
    "    print(\"No session_logs_* folder found yet.\")\n",
    "\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                   experiment   plane  n_rois_final     T  fps  tau  \\\n",
       "0   L433_f02_Exp_1_flickering  plane0           442  3340  2.0  6.0   \n",
       "1   L433_f02_Exp_1_flickering  plane1           435  3340  2.0  6.0   \n",
       "2   L433_f02_Exp_1_flickering  plane2           468  3340  2.0  6.0   \n",
       "3   L433_f02_Exp_1_flickering  plane3           372  3340  2.0  6.0   \n",
       "4   L433_f02_Exp_1_flickering  plane4           267  3340  2.0  6.0   \n",
       "..                        ...     ...           ...   ...  ...  ...   \n",
       "74   L472_f06_Exp_2_rocking_1  plane0           242  3340  2.0  6.0   \n",
       "75   L472_f06_Exp_2_rocking_1  plane1           294  3340  2.0  6.0   \n",
       "76   L472_f06_Exp_2_rocking_1  plane2           296  3340  2.0  6.0   \n",
       "77   L472_f06_Exp_2_rocking_1  plane3           230  3340  2.0  6.0   \n",
       "78   L472_f06_Exp_2_rocking_1  plane4           301  3340  2.0  6.0   \n",
       "\n",
       "                                             dFoF_dir file_prefix  \n",
       "0   C:\\Users\\suribear\\OneDrive - Université de Lau...    L433_f02  \n",
       "1   C:\\Users\\suribear\\OneDrive - Université de Lau...    L433_f02  \n",
       "2   C:\\Users\\suribear\\OneDrive - Université de Lau...    L433_f02  \n",
       "3   C:\\Users\\suribear\\OneDrive - Université de Lau...    L433_f02  \n",
       "4   C:\\Users\\suribear\\OneDrive - Université de Lau...    L433_f02  \n",
       "..                                                ...         ...  \n",
       "74  C:\\Users\\suribear\\OneDrive - Université de Lau...    L472_f06  \n",
       "75  C:\\Users\\suribear\\OneDrive - Université de Lau...    L472_f06  \n",
       "76  C:\\Users\\suribear\\OneDrive - Université de Lau...    L472_f06  \n",
       "77  C:\\Users\\suribear\\OneDrive - Université de Lau...    L472_f06  \n",
       "78  C:\\Users\\suribear\\OneDrive - Université de Lau...    L472_f06  \n",
       "\n",
       "[79 rows x 8 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment</th>\n",
       "      <th>plane</th>\n",
       "      <th>n_rois_final</th>\n",
       "      <th>T</th>\n",
       "      <th>fps</th>\n",
       "      <th>tau</th>\n",
       "      <th>dFoF_dir</th>\n",
       "      <th>file_prefix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L433_f02_Exp_1_flickering</td>\n",
       "      <td>plane0</td>\n",
       "      <td>442</td>\n",
       "      <td>3340</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>C:\\Users\\suribear\\OneDrive - Université de Lau...</td>\n",
       "      <td>L433_f02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L433_f02_Exp_1_flickering</td>\n",
       "      <td>plane1</td>\n",
       "      <td>435</td>\n",
       "      <td>3340</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>C:\\Users\\suribear\\OneDrive - Université de Lau...</td>\n",
       "      <td>L433_f02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L433_f02_Exp_1_flickering</td>\n",
       "      <td>plane2</td>\n",
       "      <td>468</td>\n",
       "      <td>3340</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>C:\\Users\\suribear\\OneDrive - Université de Lau...</td>\n",
       "      <td>L433_f02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L433_f02_Exp_1_flickering</td>\n",
       "      <td>plane3</td>\n",
       "      <td>372</td>\n",
       "      <td>3340</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>C:\\Users\\suribear\\OneDrive - Université de Lau...</td>\n",
       "      <td>L433_f02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L433_f02_Exp_1_flickering</td>\n",
       "      <td>plane4</td>\n",
       "      <td>267</td>\n",
       "      <td>3340</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>C:\\Users\\suribear\\OneDrive - Université de Lau...</td>\n",
       "      <td>L433_f02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>L472_f06_Exp_2_rocking_1</td>\n",
       "      <td>plane0</td>\n",
       "      <td>242</td>\n",
       "      <td>3340</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>C:\\Users\\suribear\\OneDrive - Université de Lau...</td>\n",
       "      <td>L472_f06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>L472_f06_Exp_2_rocking_1</td>\n",
       "      <td>plane1</td>\n",
       "      <td>294</td>\n",
       "      <td>3340</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>C:\\Users\\suribear\\OneDrive - Université de Lau...</td>\n",
       "      <td>L472_f06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>L472_f06_Exp_2_rocking_1</td>\n",
       "      <td>plane2</td>\n",
       "      <td>296</td>\n",
       "      <td>3340</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>C:\\Users\\suribear\\OneDrive - Université de Lau...</td>\n",
       "      <td>L472_f06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>L472_f06_Exp_2_rocking_1</td>\n",
       "      <td>plane3</td>\n",
       "      <td>230</td>\n",
       "      <td>3340</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>C:\\Users\\suribear\\OneDrive - Université de Lau...</td>\n",
       "      <td>L472_f06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>L472_f06_Exp_2_rocking_1</td>\n",
       "      <td>plane4</td>\n",
       "      <td>301</td>\n",
       "      <td>3340</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>C:\\Users\\suribear\\OneDrive - Université de Lau...</td>\n",
       "      <td>L472_f06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79 rows × 8 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displayed summary from: C:\\Users\\suribear\\OneDrive - Université de Lausanne\\Lab\\Data\\2p\\session_logs_20251027_173356\\summary.csv\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T13:20:38.002725Z",
     "start_time": "2025-10-28T13:20:34.970105Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ================================================================\n",
    "# Merge per-plane dFoF into a single experiment-level file\n",
    "# ================================================================\n",
    "from pathlib import Path\n",
    "import json, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "BASE_DIR = Path(r\"C:\\Users\\suribear\\OneDrive - Université de Lausanne\\Lab\\Data\\2p\")\n",
    "\n",
    "\n",
    "# EXPERIMENT_WHITELIST = [\n",
    "#     \"L433_f02_Exp_1_flickering\",\n",
    "#     \"L453_f10_Exp_1_flickering\",\n",
    "#     \"L472_f03_Exp_2_rocking_1\"\n",
    "# ]\n",
    "EXPERIMENT_WHITELIST = [\n",
    "    \"L433_f02_Exp_1_flickering\",\n",
    "    \"L433_f03_Exp_1_flickering\",\n",
    "    \"L433_f04_Exp_1_flickering\",\n",
    "    \"L433_f05_Exp_1_flickering\",\n",
    "    \"L433_f06_Exp_1_flickering\",\n",
    "    \"L453_f07_Exp_1_flickering\",\n",
    "    \"L453_f08_Exp_1_flickering\",\n",
    "    \"L453_f09_Exp_1_flickering\",\n",
    "    \"L453_f10_Exp_1_flickering\",\n",
    "    \"L453_f11_Exp_1_flickering\",\n",
    "    \"L472_f01_Exp_2_rocking_1\",\n",
    "    \"L472_f02_Exp_2_rocking_1\",\n",
    "    \"L472_f03_Exp_2_rocking_1\",\n",
    "    \"L472_f04_Exp_2_rocking_1\",\n",
    "    \"L472_f05_Exp_2_rocking_1\",\n",
    "    \"L472_f06_Exp_2_rocking_1\",\n",
    "]\n",
    "ALIGN_MODE = \"truncate\"   # or \"pad_nan\"\n",
    "DO_PLOT   = True\n",
    "PLOT_PCTL = 99.0\n",
    "\n",
    "# ---------------- HELPERS ----------------\n",
    "def ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def safe_to_csv(df, path: Path):\n",
    "    \"\"\"Write CSV safely even if file is open.\"\"\"\n",
    "    try:\n",
    "        df.to_csv(path, index=False, encoding=\"utf-8\")\n",
    "        return path\n",
    "    except PermissionError:\n",
    "        alt = path.with_name(path.stem + \"_\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\") + path.suffix)\n",
    "        df.to_csv(alt, index=False, encoding=\"utf-8\")\n",
    "        print(f\"[warn] CSV locked: wrote to {alt.name} instead of {path.name}\")\n",
    "        return alt\n",
    "\n",
    "def safe_savefig(fig, path: Path, **kwargs):\n",
    "    \"\"\"Save figure safely even if file is open.\"\"\"\n",
    "    try:\n",
    "        fig.savefig(path, **kwargs)\n",
    "        return path\n",
    "    except PermissionError:\n",
    "        alt = path.with_name(path.stem + \"_\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\") + path.suffix)\n",
    "        fig.savefig(alt, **kwargs)\n",
    "        print(f\"[warn] PNG locked: wrote to {alt.name} instead of {path.name}\")\n",
    "        return alt\n",
    "\n",
    "def find_experiments(base_dir: Path, whitelist=None):\n",
    "    out = []\n",
    "    for child in base_dir.iterdir():\n",
    "        if not child.is_dir():\n",
    "            continue\n",
    "        if whitelist and child.name not in whitelist:\n",
    "            continue\n",
    "        s2p = child / \"03_analysis\" / \"functional\" / \"suite2P\"\n",
    "        if s2p.exists() and any(p.is_dir() and p.name.startswith(\"plane\") for p in s2p.iterdir()):\n",
    "            out.append(child)\n",
    "    return sorted(out, key=lambda p: p.name)\n",
    "\n",
    "def plane_dirs(exp_dir: Path):\n",
    "    s2p = exp_dir / \"03_analysis\" / \"functional\" / \"suite2P\"\n",
    "    return sorted([p for p in s2p.iterdir() if p.is_dir() and p.name.startswith(\"plane\")], key=lambda p: p.name)\n",
    "\n",
    "def experiment_prefix(exp_name: str) -> str:\n",
    "    parts = exp_name.split(\"_\")\n",
    "    return f\"{parts[0]}_{parts[1]}\" if len(parts) >= 2 else exp_name\n",
    "\n",
    "def read_params_from_metadata(dfof_dir: Path):\n",
    "    fps = None\n",
    "    tauDecay = None\n",
    "    meta_path = dfof_dir / \"metadata.json\"\n",
    "    if meta_path.exists():\n",
    "        try:\n",
    "            with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                meta = json.load(f)\n",
    "            fps = meta.get(\"fps\", None)\n",
    "            tauDecay = meta.get(\"tauDecay\", meta.get(\"tau\", None))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return fps, tauDecay\n",
    "\n",
    "def load_dfof_for_plane(dfof_dir: Path, prefix: str):\n",
    "    for c in [dfof_dir / f\"{prefix}_dFoF.npy\", dfof_dir / f\"dFoF_{prefix}.npy\"]:\n",
    "        if c.exists():\n",
    "            return np.load(c), c.name\n",
    "    return None, None\n",
    "\n",
    "def load_filtered_indices_for_plane(dfof_dir: Path, prefix: str, N_cols: int):\n",
    "    candidates = [\n",
    "        dfof_dir / f\"{prefix}_filtered_roi_indices.npy\",\n",
    "        dfof_dir / \"filtered_roi_indices.npy\",\n",
    "        dfof_dir / \"__filtered_roi_indices.npy\",\n",
    "    ]\n",
    "    hit = None\n",
    "    for c in candidates:\n",
    "        if c.exists():\n",
    "            hit = c\n",
    "            break\n",
    "    if not hit:\n",
    "        for c in sorted(dfof_dir.glob(\"*filtered*roi*indices*.npy\")):\n",
    "            hit = c\n",
    "            break\n",
    "    if hit:\n",
    "        arr = np.load(hit)\n",
    "        if arr.ndim == 1 and arr.shape[0] == N_cols:\n",
    "            return arr.astype(int), hit.name\n",
    "        return np.arange(N_cols, dtype=int), hit.name\n",
    "    return np.arange(N_cols, dtype=int), None\n",
    "\n",
    "# ---------------- MAIN LOOP ----------------\n",
    "experiments = find_experiments(BASE_DIR, whitelist=EXPERIMENT_WHITELIST)\n",
    "print(f\"Found {len(experiments)} experiments to merge.\\n\")\n",
    "\n",
    "for exp_dir in experiments:\n",
    "    exp_name = exp_dir.name\n",
    "    prefix   = experiment_prefix(exp_name)\n",
    "    planes   = plane_dirs(exp_dir)\n",
    "    if not planes:\n",
    "        print(f\"[skip] {exp_name}: no planes found.\\n\")\n",
    "        continue\n",
    "\n",
    "    per_plane = []\n",
    "    for pdir in planes:\n",
    "        dfof_dir = pdir / \"dFoF\"\n",
    "        if not dfof_dir.exists():\n",
    "            print(f\"[warn] {exp_name} {pdir.name}: missing dFoF folder\")\n",
    "            continue\n",
    "\n",
    "        A, src_name = load_dfof_for_plane(dfof_dir, prefix)\n",
    "        if A is None:\n",
    "            print(f\"[warn] {exp_name} {pdir.name}: no dFoF file found\")\n",
    "            continue\n",
    "\n",
    "        fps, tauDecay = read_params_from_metadata(dfof_dir)\n",
    "        fi_vec, fi_src = load_filtered_indices_for_plane(dfof_dir, prefix, A.shape[1])\n",
    "        per_plane.append({\n",
    "            \"plane\": pdir.name,\n",
    "            \"dfof\": A,\n",
    "            \"filtered_indices\": fi_vec,\n",
    "            \"dfof_source\": src_name,\n",
    "            \"fi_source\": fi_src or \"\",\n",
    "            \"fps\": fps,\n",
    "            \"tauDecay\": tauDecay,\n",
    "        })\n",
    "\n",
    "    if not per_plane:\n",
    "        print(f\"[skip] {exp_name}: no dFoF arrays found in any plane.\\n\")\n",
    "        continue\n",
    "\n",
    "    # --- Align time dimension ---\n",
    "    Ts = [x[\"dfof\"].shape[0] for x in per_plane]\n",
    "    if ALIGN_MODE == \"truncate\":\n",
    "        T_common = min(Ts)\n",
    "        if len(set(Ts)) > 1:\n",
    "            print(f\"[info] {exp_name}: truncating all planes to T={T_common} (min over {Ts})\")\n",
    "        aligned = []\n",
    "        for x in per_plane:\n",
    "            y = x.copy()\n",
    "            y[\"dfof\"] = x[\"dfof\"][:T_common, :]\n",
    "            aligned.append(y)\n",
    "        T_final = T_common\n",
    "    else:\n",
    "        T_max = max(Ts)\n",
    "        aligned = []\n",
    "        for x in per_plane:\n",
    "            d = x[\"dfof\"]\n",
    "            T_i, N_i = d.shape\n",
    "            if T_i < T_max:\n",
    "                pad = np.full((T_max - T_i, N_i), np.nan, dtype=d.dtype)\n",
    "                d = np.vstack([d, pad])\n",
    "            y = x.copy()\n",
    "            y[\"dfof\"] = d\n",
    "            aligned.append(y)\n",
    "        T_final = T_max\n",
    "\n",
    "    # --- Concatenate across neurons ---\n",
    "    merged_list = []\n",
    "    mapping_rows = []\n",
    "    merged_filtered_roi_idx = []\n",
    "    col_offset = 0\n",
    "    for item in aligned:\n",
    "        d = item[\"dfof\"]\n",
    "        N = d.shape[1]\n",
    "        merged_list.append(d)\n",
    "        merged_filtered_roi_idx.extend(list(item[\"filtered_indices\"]))\n",
    "        for j in range(N):\n",
    "            mapping_rows.append({\n",
    "                \"plane\": item[\"plane\"],\n",
    "                \"roi_index_in_plane\": j,\n",
    "                \"filtered_roi_index\": int(item[\"filtered_indices\"][j]),\n",
    "                \"global_col\": col_offset + j,\n",
    "                \"source_dfof_file\": item[\"dfof_source\"],\n",
    "                \"source_fi_file\": item[\"fi_source\"],\n",
    "            })\n",
    "        col_offset += N\n",
    "\n",
    "    merged = np.concatenate(merged_list, axis=1)\n",
    "    merged_filtered_roi_idx = np.asarray(merged_filtered_roi_idx, dtype=int)\n",
    "\n",
    "    # --- Save outputs per experiment ---\n",
    "    out_merge = exp_dir / \"03_analysis\" / \"functional\" / \"suite2P\" / \"merged_dFoF\"\n",
    "    out_plots = exp_dir / \"03_analysis\" / \"functional\" / \"plots\" / \"merged_dFoF\"\n",
    "    ensure_dir(out_merge); ensure_dir(out_plots)\n",
    "\n",
    "    merged_npy = out_merge / f\"{prefix}_dFoF_merged.npy\"\n",
    "    np.save(merged_npy, merged)\n",
    "\n",
    "    merged_fi_npy = out_merge / f\"{prefix}_dFoF_merged_filtered_roi_indices.npy\"\n",
    "    np.save(merged_fi_npy, merged_filtered_roi_idx)\n",
    "\n",
    "    map_csv = out_merge / f\"{prefix}_dFoF_merged_map.csv\"\n",
    "    _ = safe_to_csv(pd.DataFrame(mapping_rows), map_csv)\n",
    "\n",
    "    # --- Plot & Save Visualization ---\n",
    "    if DO_PLOT:\n",
    "        with np.errstate(invalid=\"ignore\"):\n",
    "            col_max = np.nanmax(merged, axis=0)\n",
    "        order = np.argsort(-col_max)\n",
    "        merged_sorted = merged[:, order]\n",
    "        finite_vals = merged_sorted[np.isfinite(merged_sorted)]\n",
    "        vmax = np.percentile(finite_vals, PLOT_PCTL) if finite_vals.size else None\n",
    "        vmin = 0.0\n",
    "\n",
    "        plt.figure(figsize=(11, 6))\n",
    "        plt.imshow(merged_sorted.T, aspect=\"auto\", origin=\"lower\",\n",
    "                   cmap=\"gray_r\", vmin=vmin, vmax=vmax)\n",
    "        plt.xlabel(\"Time (frames)\")\n",
    "        plt.ylabel(\"Neuron (sorted by max ΔF/F)\")\n",
    "        plt.title(f\"{prefix} — merged dFoF raster (sorted by max)\")\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plot_png = out_plots / f\"{prefix}_dFoF_merged_raster_sorted_by_max.png\"\n",
    "        _ = safe_savefig(plt.gcf(), plot_png, dpi=150, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "\n",
    "        order_csv = out_plots / f\"{prefix}_dFoF_merged_sort_index.csv\"\n",
    "        _ = safe_to_csv(pd.DataFrame({\"plot_col_order\": order}), order_csv)\n",
    "\n",
    "        plot_map_csv = out_plots / f\"{prefix}_dFoF_merged_map_sorted.csv\"\n",
    "        _ = safe_to_csv(pd.DataFrame([mapping_rows[i] for i in order]), plot_map_csv)\n",
    "\n",
    "    print(f\"[ok] {exp_name}: merged → {merged_npy.name}, map → {map_csv.name}, \"\n",
    "          f\"fi → {merged_fi_npy.name}, plots in {out_plots.name}\\n\")\n"
   ],
   "id": "1f0a46c357191f36",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 experiments to merge.\n",
      "\n",
      "[warn] CSV locked: wrote to L433_f02_dFoF_merged_map_20251028_142035.csv instead of L433_f02_dFoF_merged_map.csv\n",
      "[warn] CSV locked: wrote to L433_f02_dFoF_merged_sort_index_20251028_142037.csv instead of L433_f02_dFoF_merged_sort_index.csv\n",
      "[warn] CSV locked: wrote to L433_f02_dFoF_merged_map_sorted_20251028_142037.csv instead of L433_f02_dFoF_merged_map_sorted.csv\n",
      "[ok] L433_f02_Exp_1_flickering: merged → L433_f02_dFoF_merged.npy, map → L433_f02_dFoF_merged_map.csv, fi → L433_f02_dFoF_merged_filtered_roi_indices.npy, plots in merged_dFoF\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T13:20:38.116639Z",
     "start_time": "2025-10-28T13:20:38.014754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==== Validate one experiment's merged dFoF ====================================\n",
    "from pathlib import Path\n",
    "import json, numpy as np, pandas as pd\n",
    "\n",
    "# ---- Set these ----\n",
    "BASE_DIR  = Path(r\"C:\\Users\\suribear\\OneDrive - Université de Lausanne\\Lab\\Data\\2p\")\n",
    "EXP_NAME  = \"L433_f03_Exp_1_flickering\"     # change if you want another\n",
    "SAMPLE_COLS = 100                           # number of merged columns to spot-check (<= N_total)\n",
    "\n",
    "# ---- Helpers ----\n",
    "def experiment_prefix(exp_name: str) -> str:\n",
    "    parts = exp_name.split(\"_\")\n",
    "    return f\"{parts[0]}_{parts[1]}\" if len(parts) >= 2 else exp_name\n",
    "\n",
    "def plane_dirs(exp_dir: Path):\n",
    "    s2p = exp_dir / \"03_analysis\" / \"functional\" / \"suite2P\"\n",
    "    return sorted([p for p in s2p.iterdir() if p.is_dir() and p.name.startswith(\"plane\")], key=lambda p: p.name)\n",
    "\n",
    "def find_merged_dir(exp_dir: Path, prefix: str):\n",
    "    # prefer suite2P/merged_dFoF; fallback to functional/plots/merged_dFoF\n",
    "    candidates = [\n",
    "        exp_dir / \"03_analysis\" / \"functional\" / \"suite2P\" / \"merged_dFoF\",\n",
    "        exp_dir / \"03_analysis\" / \"functional\" / \"plots\"   / \"merged_dFoF\",\n",
    "    ]\n",
    "    for c in candidates:\n",
    "        if (c / f\"{prefix}_dFoF_merged.npy\").exists():\n",
    "            return c\n",
    "    # default to first candidate\n",
    "    return candidates[0]\n",
    "\n",
    "def load_dfof_for_plane(dfof_dir: Path, prefix: str):\n",
    "    for c in [dfof_dir / f\"{prefix}_dFoF.npy\", dfof_dir / f\"dFoF_{prefix}.npy\"]:\n",
    "        if c.exists():\n",
    "            return np.load(c), c.name\n",
    "    return None, None\n",
    "\n",
    "def load_filtered_indices_for_plane(dfof_dir: Path, prefix: str, N_cols: int):\n",
    "    # common names\n",
    "    candidates = [\n",
    "        dfof_dir / f\"{prefix}_filtered_roi_indices.npy\",\n",
    "        dfof_dir / \"filtered_roi_indices.npy\",\n",
    "        dfof_dir / \"__filtered_roi_indices.npy\",\n",
    "    ]\n",
    "    hit = None\n",
    "    for c in candidates:\n",
    "        if c.exists():\n",
    "            hit = c; break\n",
    "    if not hit:\n",
    "        # any *filtered*roi*indices*.npy\n",
    "        for c in sorted(dfof_dir.glob(\"*filtered*roi*indices*.npy\")):\n",
    "            hit = c; break\n",
    "    if hit:\n",
    "        arr = np.load(hit)\n",
    "        if arr.ndim == 1 and arr.shape[0] == N_cols:\n",
    "            return arr.astype(int)\n",
    "    # fallback: identity vector [0..N-1]\n",
    "    return np.arange(N_cols, dtype=int)\n",
    "\n",
    "def approx_equal(a, b, rtol=1e-6, atol=1e-8):\n",
    "    # compare allowing NaNs (treat NaN==NaN)\n",
    "    mask = ~(np.isnan(a) & np.isnan(b))\n",
    "    return np.allclose(a[mask], b[mask], rtol=rtol, atol=atol)\n",
    "\n",
    "# ---- Locate experiment & merged outputs ----\n",
    "exp_dir = BASE_DIR / EXP_NAME\n",
    "prefix  = experiment_prefix(EXP_NAME)\n",
    "planes  = plane_dirs(exp_dir)\n",
    "if not planes:\n",
    "    raise RuntimeError(\"No plane* dirs found.\")\n",
    "\n",
    "merged_dir = find_merged_dir(exp_dir, prefix)\n",
    "merged      = np.load(merged_dir / f\"{prefix}_dFoF_merged.npy\")                         # (T_merged, N_total)\n",
    "merged_roi  = np.load(merged_dir / f\"{prefix}_dFoF_merged_filtered_roi_indices.npy\")    # (N_total,)\n",
    "map_df      = pd.read_csv(merged_dir / f\"{prefix}_dFoF_merged_map.csv\")                 # global_col, plane, roi_index_in_plane, filtered_roi_index, ...\n",
    "\n",
    "T_merged, N_total = merged.shape\n",
    "assert merged_roi.shape == (N_total,), f\"ROI index vector length {merged_roi.shape[0]} != N_total {N_total}\"\n",
    "assert map_df.shape[0] == N_total, f\"Map rows {map_df.shape[0]} != N_total {N_total}\"\n",
    "\n",
    "print(f\"[info] merged shape = {merged.shape}, map rows = {len(map_df)}\")\n",
    "\n",
    "# ---- Gather per-plane raw dFoF + filtered indices ----\n",
    "per_plane = []\n",
    "Ts = []\n",
    "for pdir in planes:\n",
    "    dfof_dir = pdir / \"dFoF\"\n",
    "    A, src = load_dfof_for_plane(dfof_dir, prefix)\n",
    "    if A is None:\n",
    "        print(f\"[warn] {pdir.name}: missing dFoF, skipping for validation\")\n",
    "        continue\n",
    "    Ts.append(A.shape[0])\n",
    "    fi = load_filtered_indices_for_plane(dfof_dir, prefix, A.shape[1])\n",
    "    per_plane.append({\"plane\": pdir.name, \"A\": A, \"fi\": fi})\n",
    "\n",
    "if not per_plane:\n",
    "    raise RuntimeError(\"No usable dFoF arrays found to validate against.\")\n",
    "\n",
    "# ---- Detect alignment mode ----\n",
    "T_min, T_max = min(Ts), max(Ts)\n",
    "if T_merged == T_min:\n",
    "    align_mode = \"truncate\"\n",
    "elif T_merged == T_max:\n",
    "    align_mode = \"pad_nan\"\n",
    "else:\n",
    "    align_mode = f\"custom ({T_merged=} vs min={T_min}, max={T_max})\"\n",
    "print(f\"[info] detected align mode: {align_mode}\")\n",
    "\n",
    "# ---- Rebuild the expected merged ROI index vector from planes and compare ----\n",
    "expected_roi = []\n",
    "for item in per_plane:\n",
    "    expected_roi.extend(list(item[\"fi\"]))\n",
    "expected_roi = np.asarray(expected_roi, dtype=int)\n",
    "\n",
    "if expected_roi.shape[0] != N_total:\n",
    "    print(f\"[warn] expected ROI vector length {expected_roi.shape[0]} != N_total {N_total} \"\n",
    "          f\"(did some planes miss dFoF during validation?)\")\n",
    "\n",
    "# Map can come from fewer planes if some were missing; compare overlapping length\n",
    "L = min(expected_roi.shape[0], merged_roi.shape[0])\n",
    "roi_match = np.array_equal(expected_roi[:L], merged_roi[:L])\n",
    "print(f\"[check] merged_filtered_roi_indices match expected (first {L}): {roi_match}\")\n",
    "\n",
    "# Also check the CSV's 'filtered_roi_index' equals merged_roi\n",
    "csv_match = np.array_equal(map_df[\"filtered_roi_index\"].values[:L], merged_roi[:L])\n",
    "print(f\"[check] map CSV filtered_roi_index matches merged vector (first {L}): {csv_match}\")\n",
    "\n",
    "# ---- Spot-check that merged columns equal the correct plane ROI columns ----\n",
    "# Choose up to SAMPLE_COLS valid columns\n",
    "rng = np.random.default_rng(0)\n",
    "sample_cols = np.arange(N_total) if N_total <= SAMPLE_COLS else rng.choice(N_total, SAMPLE_COLS, replace=False)\n",
    "\n",
    "fail_count = 0\n",
    "for k in sample_cols:\n",
    "    row = map_df.iloc[k]\n",
    "    plane_name = row[\"plane\"]\n",
    "    roi_in_plane = int(row[\"roi_index_in_plane\"])\n",
    "    # find plane array\n",
    "    match = next((x for x in per_plane if x[\"plane\"] == plane_name), None)\n",
    "    if match is None:\n",
    "        print(f\"[warn] plane {plane_name} not found in validation set; skipping col {k}\")\n",
    "        continue\n",
    "    A = match[\"A\"]\n",
    "    col_src = A[:, roi_in_plane]\n",
    "    # align time\n",
    "    if align_mode == \"truncate\":\n",
    "        col_src = col_src[:T_merged]\n",
    "    elif align_mode.startswith(\"pad_nan\"):\n",
    "        if len(col_src) < T_merged:\n",
    "            pad = np.full(T_merged - len(col_src), np.nan, dtype=col_src.dtype)\n",
    "            col_src = np.concatenate([col_src, pad], axis=0)\n",
    "    # compare\n",
    "    ok = approx_equal(merged[:, k], col_src)\n",
    "    if not ok:\n",
    "        fail_count += 1\n",
    "        if fail_count <= 10:\n",
    "            print(f\"[mismatch] col {k}: plane={plane_name}, roi={roi_in_plane}\")\n",
    "\n",
    "print(f\"[summary] columns checked: {len(sample_cols)}, mismatches: {fail_count}\")\n",
    "print(\"PASS\" if roi_match and csv_match and fail_count == 0 else \"CHECK WARNINGS ABOVE\")\n",
    "\n"
   ],
   "id": "225c76f5c3a0eea4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] merged shape = (3340, 1533), map rows = 1533\n",
      "[info] detected align mode: truncate\n",
      "[check] merged_filtered_roi_indices match expected (first 1533): True\n",
      "[check] map CSV filtered_roi_index matches merged vector (first 1533): True\n",
      "[summary] columns checked: 100, mismatches: 0\n",
      "PASS\n"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
